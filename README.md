
# Tutorials über Maschinelles Lernen, Künstliche Intelligenz und Data Science

## Schritt für Schritt zum Mathemagier mit neuromant und Python

Hier finden sich interaktive Tutorials, die in losen Abständen auf dem Blog [neuromant.de](https://neuromant.de) veröffentlicht werden. 

Dort findet sich auch das [Impressum](https://neuromant.de/about) für dieses Github-Repository.

Alle Tutorials gibt es zum besseren Lesen online unter [https://neuromant.de/tutorials](https://neuromant.de/tutorials) - zum Ausprobieren zu Hause sind die Jupyter Notebooks deutlich besser geeignet ;-)

## Systemvoraussetzungen

Für die Tutorials wird eine Installation von Python 3.x mit Jupyter Notebook vorausgesetzt. 

Für Einsteiger ist die Distribution von [Anaconda Python](https://www.anaconda.com/downloads) empfohlen. Diese gibt es für Linux, Windows und Macintosh.

## Tutorial-Reihe 1: Das Perzeptron - Neuronale Netze für Einsteiger und Fortgeschrittene

Die erste Tutorial-Reihe motiviert das sogenannte Perzeptron: from scratch wird hier mit Python-Code nach und nach das vollständige mehrschichtige Perzeptron (engl. multilayer perceptron, MLP) motiviert und implementiert. Notwendige Mathematik wird in einfacher Sprache erklärt und vertieft, so dass alle Formeln sowohl gelesen als auch kapiert werden können.

#### [Das Perzeptron - Teil 1](/notebooks/Tutorial_Das-Perzeptron-Teil-1.ipynb)

<img src="/notebooks/images/perzeptron-einfaches-modell-1.png" width="200" align="right"/>

*Neuronales Netz selbst entwickeln mit Python*

Intuitive Einführung in das Perzeptron - in wenigen Zeilen Code entsteht pure Magie: wir schauen hinter das Geheimnis einfacher neuronaler Netze

Stichwörter: Delta-Regel, Heaviside-Aktivierung, Skalarprodukt

#### [Das Perzeptron - Teil 2](/notebooks/Tutorial_Das-Perzeptron-Teil-2.ipynb)

<img src="/notebooks/images/perzeptron-2-sonardisplay.png" width="200" align="right"/>

*Mit Python und Numpy das Perzeptron verstehen*

Eine richtige Anwendung für unser Perzeptron: Wir untersuchen Daten eines Sonars auf wiederkehrende Muster und erarbeiten dazu eine gute Trainingsstrategie

Stichwörter: Overfitting, Überanpassung, Generalisierung

#### [Das Perzeptron - Teil 3](/notebooks/Tutorial_Das-Perzeptron-Teil-3.ipynb)

<img src="/notebooks/images/perzeptron-3-parabel-steigung.png" width="200" align="right"/>

*Gradientenabstieg und Herleitung der Delta-Regel - die Mathematik hinter den Neuronalen Netzen*

Learn Artificial Intelligence with one weird trick - was steckt wirklich hinter dem Training neuronaler Netze? Anspruchsvolles und umfangreiches Tutorial; die notwendige Mathematik wird aufgefrischt.

Stichwörter: Gradientenabstieg (engl. gradient descent), Verlustfunktionen (engl. loss functions), Kettenregel

#### [Das Perzeptron - Teil 4](/notebooks/Tutorial_Das-Perzeptron-Teil-4.ipynb)

<img src="/notebooks/images/perzeptron-b-sorcerer.png" width="200" align="right"/>

*Der Lego-Baukasten des Maschinellen Lernens - heute mit sigmoidaler Aktivierungsfunktion*

Bevor wir weiter in die Tiefe gehen, schauen wir einmal in die Breite: mit der sigmoiden Aktivierungsfunktion und einer dazu passenden Verlustfunktion verwandelt sich das Perzeptron in eine logistische Regression - die wir gleich für die Klassifikation in gute vs. schlechte Weine einsetzen.

Stichwörter: logistische Regression, log-loss, sigmoide Aktivierung

## Feedback erwünscht

Feedback ist gerne gesehen. Hinterlasst einen Github-Stern, schreibt dem Autoren oder startet eine Diskussion - hier oder im Blog.

Fehler gefunden? Dann gerne melden, wer möchte, wird in die **neuromant "Hall of Fame"** aufgenommen.

Viel Spaß beim Ausprobieren!

Danny Busch | Blog: [neuromant.de](https://neuromant.de)